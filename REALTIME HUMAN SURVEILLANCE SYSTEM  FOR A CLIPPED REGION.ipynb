{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import imutils\n",
    "import glob\n",
    "from sendgrid.helpers.mail import *\n",
    "from sendgrid import SendGridAPIClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting working directory\n",
    "os.chdir(r'xyz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n"
     ]
    }
   ],
   "source": [
    "# Reading all the availabe labels available for detection.\n",
    "# We are interested in Human Objects only and that is available in the list ==> \"Person\" Category\n",
    "LABELS = []\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    LABELS = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "print(LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using cv2 to read the pretrained YOLO algorithm and its weights.\n",
    "net = cv2.dnn.readNetFromDarknet('yolov3.cfg', 'yolov3.weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting layers of YOLO\n",
    "layer_names = net.getLayerNames()\n",
    "ln = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the SORT Algorithm for tracking\n",
    "from SORT import *\n",
    "tracker = Sort()\n",
    "memory = {}\n",
    "cross_check = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-292e28722658>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mlayerOutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mln\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Defining the surveillance window\n",
    "COLORS = np.random.uniform(0, 255, size=(len(LABELS), 3))\n",
    "window = np.array([[[0,400 ], [500, 400], [500, 575], [0, 575]]], np.int32) \n",
    "# This window can be changed by changing the coordinates\n",
    "\n",
    "\n",
    "# Video Feed input\n",
    "cam = cv2.VideoCapture(\"Sample.mp4\")\n",
    "# cam = cv2.VideoCapture(0) ==> Use this command to access the laptop webcam feed\n",
    "cam.read()\n",
    "cam.set(3,640)\n",
    "cam.set(4,480)\n",
    "writer = None\n",
    "(W, H) = (None, None)\n",
    "\n",
    "frameIndex = 0\n",
    "\n",
    "\n",
    "# Looping for the Detection Process\n",
    "while True:\n",
    "    # Read the next frame from the file\n",
    "    (grabbed, frame) = cam.read()\n",
    "    \n",
    "    # If the frame was not grabbed, then we have reached the end of the stream\n",
    "    if not grabbed:\n",
    "        break\n",
    "    \n",
    "    # If the frame dimensions are empty, grab them\n",
    "    if W is None or H is None:\n",
    "        (H, W) = frame.shape[:2]\n",
    "\n",
    "    # Construct a blob from the input frame and then perform a forward pass of the YOLO object detector, \n",
    "    # giving us our bounding boxes and associated probabilities\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    start = time.time()\n",
    "    layerOutputs = net.forward(ln)\n",
    "    end = time.time()\n",
    "\n",
    "    # Initialize our lists of detected bounding boxes, confidences,and class IDs, respectively\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    classIDs = []\n",
    "\n",
    "    # loop over each of the layer outputs\n",
    "    for output in layerOutputs:\n",
    "        # loop over each of the detections\n",
    "        for detection in output:\n",
    "            # Extract the class ID and confidence (i.e., probability) of the current object detection\n",
    "            scores = detection[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "\n",
    "            # Filter out weak predictions by ensuring the detected probability is greater than the minimum probability\n",
    "            if confidence > 0.5:\n",
    "                # Scale the bounding box coordinates back relative to the size of the image, \n",
    "                # keeping in mind that YOLO actually returns the center (x, y)-coordinates of the bounding box \n",
    "                # followed by the boxes' width and height\n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "                # Use the center (x, y)-coordinates to derive the top and left corner of the bounding box\n",
    "                x = int(centerX - (width / 2))\n",
    "                y = int(centerY - (height / 2))\n",
    "                \n",
    "                # Update our list of bounding box coordinates, confidences only for the class: 0 i.e. \"person\" class\n",
    "                if classID == 0:\n",
    "                    boxes.append([x, y, int(width), int(height)])\n",
    "                    confidences.append(float(confidence))\n",
    "                    classIDs.append(classID)\n",
    "\n",
    "    # Creating the frame/region for surveillance\n",
    "    cv2.polylines(frame, [window], True, (255,120,255),5)\n",
    "    \n",
    "    # Apply non-maxima suppression to suppress weak, overlapping bounding boxes\n",
    "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.3)\n",
    "\n",
    "    # Ensure at least one detection exists\n",
    "    dets = []\n",
    "    if len(idxs) > 0:\n",
    "        # loop over the indexes we are keeping\n",
    "        for i in idxs.flatten():\n",
    "            # Extract the bounding box coordinates\n",
    "            (x, y) = (boxes[i][0], boxes[i][1])\n",
    "            (w, h) = (boxes[i][2], boxes[i][3])\n",
    "            dets.append([x, y, x + w, y + h, confidences[i]])\n",
    "\n",
    "    # Passing each box through SORT for tracking the person\n",
    "    np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "    dets = np.asarray(dets)\n",
    "    tracks = tracker.update(dets)\n",
    "\n",
    "    boxes = []\n",
    "    indexIDs = []\n",
    "    c = []\n",
    "    previous = memory.copy()\n",
    "    memory = {}\n",
    "\n",
    "    for track in tracks:\n",
    "        boxes.append([track[0], track[1], track[2], track[3]])\n",
    "        indexIDs.append(int(track[4]))\n",
    "\n",
    "    # Draw a bounding box rectangle and label on the frame\n",
    "    if len(boxes) > 0:\n",
    "        i = int(0)\n",
    "        for box in boxes:\n",
    "            (x1, y1) = (int(box[0]), int(box[1]))\n",
    "            (x2, y2) = (int(box[2]), int(box[3]))\n",
    "            \n",
    "            bottom_r_x = x2\n",
    "            bottom_r_y = y2\n",
    "            bottom_l_x = x1\n",
    "            bottom_l_y = y2\n",
    "            \n",
    "            color = [int(c) for c in COLORS[classIDs[i]]]\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "\n",
    "            # Raising an Alarm using the SendGrid API\n",
    "            if (bottom_l_x<500 and bottom_l_x>0 and bottom_l_y<575 and bottom_l_y>400) or (bottom_r_x<500 and bottom_r_x>0 and bottom_r_y<575 and bottom_r_y>400):\n",
    "                if indexIDs[i] not in cross_check:\n",
    "                    message = Mail(from_email='HumanDetection@Alarm.com',\n",
    "                              to_emails='abc@xyz.com', # Enter your personal email id\n",
    "                              subject='Human Detected',\n",
    "                              html_content='<strong>Please check your webcam feed</strong>')\n",
    "\n",
    "                    sg = SendGridAPIClient(\"xyz\") # Enter the api key instead of xyz\n",
    "                    response = sg.send(message)\n",
    "                    cross_check.append(indexIDs[i])\n",
    "            \n",
    "            # Labelling the Object Frame\n",
    "            text = \"{}: {:.4f}\".format(LABELS[classIDs[i]], indexIDs[i])\n",
    "            cv2.putText(frame, text, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "            i += 1\n",
    "     \n",
    "    frameIndex += 1\n",
    "\n",
    "    # Output Feed\n",
    "    cv2.imshow('camera',frame)\n",
    "    \n",
    "    k = cv2.waitKey(1)\n",
    "    if k == 27:\n",
    "        break\n",
    "        \n",
    "# Interrupt the kernel to stop the detection process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [INFO] Exiting Program and cleanup stuff\n"
     ]
    }
   ],
   "source": [
    "print('\\n [INFO] Exiting Program and cleaning up')\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
